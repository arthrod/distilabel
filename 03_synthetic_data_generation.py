# -*- coding: utf-8 -*-
"""03-synthetic-data-generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/davanstrien/data-for-fine-tuning-llms/blob/main/03-synthetic-data-generation.ipynb

# Synthethic data generation

You can use LLMs to generate training data for fine-tuning LLMs. As you migh expect, the generated data is not as good as the real data, but it can be useful to bootstrap your data for fine-tuning LLMs. In this tutorial, we provide easy and simple examples to generate synthetic data using LLMs, but given the architecture of `distilabel` it is easy to scale this to way more complex pipelines and larger workloads.

## Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# install from develop because of small bug EvolQuality distilabel<1.2
# %pip install "git+https://github.com/argilla-io/distilabel.git@develop#egg=distilabel[openai]" -qq

"""## Load dataset

We will use the dataset from our [Data is Better Together](https://github.com/huggingface/data-is-better-together). Data is Better Together is a collaboration between ðŸ¤— Hugging Face, ðŸ“ Argilla, and the Open-Source ML community. We aim to empower the open-source community to build impactful datasets collectively.

This prompt ranking dataset was created by applying human evaluation in prompt, where roughly 400 people annotated human and synthehtic prompt to asses their quality on a scale from one to 5.
"""

from datasets import load_dataset

dataset = load_dataset("DIBT/10k_prompts_ranked")
dataset

dataset["train"][0]

"""## Load LLMs

We will now load a LLM integration within distilabel. For ease, we will use the `OpenAILLM`. Practically, there are two things to consider.

- You can customize use any propietary or open-source LLM you want to use for vendor lock-in and licensing reasons.
- Each LLM integration has its own arguments which are inherited from the original LLM provider.
- You might want to use different LLMs providers for different steps in the pipeline for diversity and quality reasons.
- You might need to set `OPENAI_API_KEY` in your environment variables to use the LLMs.
"""

from distilabel.llms import OpenAILLM

llm = OpenAILLM(model="gpt-4")

"""## Synthesizing Generations

We will now generate synthetic data using the LLM. We will use different prompt templates that were introduced and evaluated in various research papers. We call these `Tasks` within `distilabel`. Practically, there are several things to consider.

- `Tasks` are and are not exhaustive. You can create your own `Tasks` based on your use-case.
- `Tasks` are merely based on research papers and not always exact reproductions.

### Generate responses with `TextGeneration`

We don't always need to use complex verified prompts. We can also just go for basic chat completion. Practically, there are several things to consider.

- You might need to do some pre or post-processing to enure the data is formatted correctly.
- If working with chat data, you might want to use the `ChatGeneration` task instead.
"""

from distilabel.steps.tasks import TextGeneration

text_generation = TextGeneration(name="text_generation", llm=llm)

from distilabel.pipeline import Pipeline
from distilabel.steps import LoadHubDataset, KeepColumns
from distilabel.steps.tasks import TextGeneration

with Pipeline(name="text_generation") as pipeline_text_generation:
    load_hub_dataset = LoadHubDataset(
        name="load_dataset",
        repo_id="DIBT/10k_prompts_ranked",
        output_mappings={"prompt": "instruction"},
        split="train",
        num_examples=1,
    )
    text_generation = TextGeneration(name="text_generation", llm=llm)
    keep_columns = KeepColumns(
        name="keep_columns",
        columns=[
            "instruction",
            "generation",
        ],
    )
    load_hub_dataset >> text_generation >> keep_columns
# 1
distiset_text_generation = pipeline_text_generation.run()

distiset_text_generation["default"]["train"][0]

"""### Improve prompts with `SelfInstruct`

Based on the paper [Self-Instruct: Aligning LM with Self Generated Instructions](https://arxiv.org/abs/2212.10560). It relies on rewriting an instruction based on certain critaria that are deemed important. Practically, there are several things to consider.

- You might customize `criteria_for_query_generation` to improve the quality of the prompts to you domain.
"""

### COMMENTED OUT BECAUSE IT REQUIRES RELOADING THE NOTEBOOK AND LLM ###
# from distilabel.steps.tasks import SelfInstruct

# self_instruct = SelfInstruct(name="self_instruct", llm=llm, num_instructions=1)
# self_instruct.load()
# self_instruct._template.render()

from distilabel.pipeline import Pipeline
from distilabel.steps import LoadHubDataset, KeepColumns
from distilabel.steps.tasks import SelfInstruct

with Pipeline(name="self_instruct") as pipeline_self_instruct:
    load_hub_dataset = LoadHubDataset(
        name="load_dataset",
        repo_id="DIBT/10k_prompts_ranked",
        output_mappings={"prompt": "input"},
        split="train",
        num_examples=1,
    )
    self_instruct = SelfInstruct(name="self_instruct", llm=llm, num_instructions=1)
    keep_columns = KeepColumns(
        name="keep_columns",
        columns=[
            "input",
            "instructions",
        ],
    )
    load_hub_dataset >> self_instruct >> keep_columns

distiset_self_instruct = pipeline_self_instruct.run()

distiset_self_instruct["default"]["train"][0]

"""### Improve responses with `EvolQuality`

Based on the paper [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/pdf/2312.15685). It relies on an LLM to improve the quality response given an input based on different criteria. Practically, there are several things to consider.

- You might want to directly link this pipeline with the `TextGeneration` step.
- You might want to generate with `num_evolutions>1` so we directly have more than two options for preference data annotation
"""

from distilabel.steps.tasks import EvolQuality

evol_quality = EvolQuality(name="evol_quality", llm=llm, num_evolutions=1)
evol_quality.mutation_templates

from distilabel.pipeline import Pipeline
from distilabel.steps import LoadDataFromDicts, KeepColumns
from distilabel.steps.tasks import EvolQuality

with Pipeline(name="evol_quality") as pipeline_evol_quality:
    load_data = LoadDataFromDicts(
        name="load_data",
        data=[
            {
                "instruction": "What did Leonarda Da Vinci focus on during his life?",
                "response": "He was an Italian scientist and engineer in the renaissance.",
            }
        ],
    )
    evol_quality = EvolQuality(name="evol_quality", llm=llm, num_evolutions=1)
    keep_columns = KeepColumns(
        name="keep_columns",
        columns=[
            "instruction",
            "response",
            "evolved_response",
        ],
    )
    load_data >> evol_quality >> keep_columns

distiset_evol_quality = pipeline_evol_quality.run()

distiset_evol_quality["default"]["train"][0]

"""## Synthesizing AI Feedback with LLMs as Judges

We will now generate synthetic evaluations using the LLM. This once again relies on prompt templates and `Tasks` as with synthesizing generations. Practically, there are several things to consider.

- Only several LLMs can actually generate evaluations that align with human evaluations. Higher-end propietary models from companies like `mistral` and `OpenAI` are better at this. For open-source models, you might want to use `Prometheus 2.0`.
- You can use evaluate based on differnt aspects like helpfulness, relevance, and fluency, however, given the cost, an overall rating is usually sufficient.

### Absolute evaluation of reponses using `UltraFeedback`

Based on the paper [UltraFeedback: Boosting Language Models with High-quality Feedback](https://arxiv.org/abs/2310.01377). In this case, we will generate an absolute feedback score based on an overall rating. Practically, there are several things to consider.

- A single overall rating is usually sufficient but UltraFeedback also covers (Instruction-following, truthfullness, honesty, helpfulness).
- `PrometheusEval` and `Prometheus 2.0` are a good alternative for open-source models.
"""

from distilabel.steps.tasks import UltraFeedback

ultra_feedback = UltraFeedback(name="ultra_feedback", llm=llm, aspect="overall-rating")
ultra_feedback._system_prompt

from distilabel.pipeline import Pipeline
from distilabel.steps import LoadDataFromDicts, KeepColumns
from distilabel.steps.tasks import UltraFeedback

with Pipeline(name="ultra_feedback") as pipeline_ultra_feedback:
    load_data = LoadDataFromDicts(
        name="load_data",
        data=[
            {
                "instruction": "What did Leonarda Da Vinci focus on during his life?",
                "generations": [
                    "He was an Italian scientist and engineer in the renaissance.",
                    "He was a painter, sculptor, architect, and engineer.",
                ],
            }
        ],
    )
    ultra_feedback = UltraFeedback(
        name="ultra_feedback", llm=llm, aspect="overall-rating"
    )
    keep_columns = KeepColumns(
        name="keep_columns",
        columns=["instruction", "generations", "ratings", "rationales"],
    )
    load_data >> ultra_feedback >> keep_columns

distiset_ultra_feedback = pipeline_ultra_feedback.run()

distiset_ultra_feedback["default"]["train"][0]

"""### Relative evaluation (ranking) of responses using ranking `QualityScorer`

Based on the paper [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/pdf/2312.15685). Practically, there are several things to consider.

- You might want use the `PairRM` model for predictive open source alternative to LLMs as relative judges.

"""

### COMMENTED OUT BECAUSE IT REQUIRES RELOADING THE NOTEBOOK AND LLM ###
# from distilabel.steps.tasks import QualityScorer

# quality_scorer = QualityScorer(name="quality_scorer", llm=llm)
# quality_scorer.load()
# quality_scorer._template.render()

from distilabel.pipeline import Pipeline
from distilabel.steps import LoadDataFromDicts, KeepColumns
from distilabel.steps.tasks import QualityScorer

with Pipeline(name="quality_scorer") as pipeline_quality_scorer:
    load_data = LoadDataFromDicts(
        name="load_data",
        data=[
            {
                "instruction": "What did Leonarda Da Vinci focus on during his life?",
                "responses": [
                    "He was an Italian scientist and engineer in the renaissance.",
                    "He was a painter, sculptor, architect, and engineer.",
                ],
            }
        ],
    )
    quality_scorer = QualityScorer(name="quality_scorer", llm=llm)
    keep_columns = KeepColumns(
        name="keep_columns",
        columns=[
            "instruction",
            "responses",
            "scores",
        ],
    )
    load_data >> quality_scorer >> keep_columns

distiset_quality_scorer = pipeline_quality_scorer.run()

distiset_quality_scorer["default"]["train"][0]

"""## A full pipeline

We will now combine all the steps into a full pipeline. Practically, there are several things to consider.

- You can be as creative as you want with the pipeline.
- More complex pipelines generally require more computational resources, results are cached but it can still be expensive.
"""

from distilabel.pipeline import Pipeline
from distilabel.steps import LoadHubDataset, KeepColumns
from distilabel.steps.tasks import TextGeneration, EvolQuality, QualityScorer

with Pipeline(name="text_generation") as pipeline_complete:
    load_hub_dataset = LoadHubDataset(
        name="load_dataset",
        repo_id="DIBT/10k_prompts_ranked",
        output_mappings={"prompt": "instruction"},
        split="train",
        num_examples=4,
    )
    text_generation = TextGeneration(name="text_generation", llm=llm)
    evol_quality = EvolQuality(
        name="evol_quality",
        llm=llm,
        num_evolutions=2,
        store_evolutions=True,
        input_mappings={"response": "generation"},
    )
    quality_scorer = QualityScorer(
        name="quality_scorer",
        llm=llm,
        input_mappings={"responses": "evolved_responses"},
    )
    keep_columns = KeepColumns(
        name="keep_columns",
        columns=["instruction", "evolved_responses", "scores"],
    )
    (
        load_hub_dataset
        >> text_generation
        >> evol_quality
        >> quality_scorer
        >> keep_columns
    )


distiset_complete = pipeline_complete.run()

distiset_complete["default"]["train"][:]

"""## Resources

Additinally, you can find more information on the following resources.

- [Distilabel 1.0 launch](https://argilla.io/blog/introducing-distilabel-1/)
- [Datasets on the Hugging Face hub](https://huggingface.co/datasets?other=distilabel&sort=trending)
- [Paper Implementations](https://distilabel.argilla.io/latest/sections/pipeline_samples/papers/)
"""
